{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/programmerPrati/LLM_Inference_vLLM/blob/main/LLM_inference_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clean up and install specific compatible versions\n",
        "# !pip install --upgrade click==8.1.7  # Fixes the rasterio conflict\n",
        "# !pip install vllm>=0.6.3 apache-beam[gcp]==2.61.0 nest_asyncio openai\n",
        "\n",
        "!pip install openai>=1.52.2\n",
        "!pip install vllm>=0.6.3\n",
        "!pip install triton>=3.1.0\n",
        "!pip install apache-beam[gcp]==2.61.0\n",
        "!pip install nest_asyncio # only needed in colab\n",
        "!pip check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY3NZO_FE_Y0",
        "outputId": "36ef537c-1b05-46c0-fa88-3c609d9af509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache-beam==2.61.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.61.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (1.7)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (3.11.5)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (1.12.1)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (0.20)\n",
            "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (1.65.5)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2.7.3)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (4.25.1)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (3.4.2)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.14.3 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2.0.2)\n",
            "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (0.7.1)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (25.0)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (4.15.5)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (5.29.5)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2025.2)\n",
            "Requirement already satisfied: redis<6,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (5.3.1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2025.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2.32.4)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (4.15.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (0.25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (6.0.3)\n",
            "Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.12/dist-packages (from apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (0.7)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (5.5.2)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.28.1)\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (0.5.31)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (0.2.1)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.21.0)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.34.0)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (1.13.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.18.2 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (3.38.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.35.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.5.0)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.34.0)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (3.60.0)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (3.33.0)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.18.0)\n",
            "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (2.17.0)\n",
            "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (3.11.0)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (0.10.18)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (1.130.0)\n",
            "Requirement already satisfied: keyrings.google-artifactregistry-auth in /usr/local/lib/python3.12/dist-packages (from apache-beam[gcp]==2.61.0) (1.1.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]==2.61.0) (1.72.0)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.12/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.61.0) (4.1.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.61.0) (1.17.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]==2.61.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]==2.61.0) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (1.15.0)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (2.1.2)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (1.55.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (2.12.3)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (0.17.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]==2.61.0) (2.8.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.12.4 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]==2.61.0) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0.0dev,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]==2.61.0) (1.7.1)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.61.0) (1.65.5)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.61.0) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.27.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.61.0) (1.37.0)\n",
            "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]==2.61.0) (7.7.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.61.0) (0.5.4)\n",
            "Requirement already satisfied: grpc-interceptor>=0.15.4 in /usr/local/lib/python3.12/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.61.0) (0.15.4)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions>=0.43b0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.61.0) (0.58b0)\n",
            "Requirement already satisfied: opentelemetry-resourcedetector-gcp>=1.8.0a0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.61.0) (1.11.0a0)\n",
            "Requirement already satisfied: google-cloud-monitoring>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.61.0) (2.28.0)\n",
            "Requirement already satisfied: mmh3>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.61.0) (5.2.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.12/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (0.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (3.2.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (0.30.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2.8.0)\n",
            "Requirement already satisfied: PyJWT>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from redis<6,>=5.0.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2.10.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.61.0->apache-beam[gcp]==2.61.0) (2025.11.12)\n",
            "Requirement already satisfied: keyring in /usr/local/lib/python3.12/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (25.7.0)\n",
            "Requirement already satisfied: pluggy in /usr/local/lib/python3.12/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (1.6.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (4.12.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (1.3.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.61.0) (0.6.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.61.0) (8.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (0.4.2)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (3.5.0)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (4.3.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (6.0.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.61.0) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.61.0) (3.23.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.12/dist-packages (from SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (10.8.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.61.0) (2.23)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "ipython 7.34.0 requires jedi, which is not installed.\n",
            "rasterio 1.4.4 has requirement click!=8.2.*,>=4.0, but you have click 8.2.1.\n",
            "dask 2025.9.1 has requirement cloudpickle>=3.0.0, but you have cloudpickle 2.2.1.\n",
            "multiprocess 0.70.16 has requirement dill>=0.3.8, but you have dill 0.3.1.1.\n",
            "distributed 2025.9.1 has requirement cloudpickle>=3.0.0, but you have cloudpickle 2.2.1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import torch\n",
        "\n",
        "def cleanup_vllm():\n",
        "    # 1. Kill any processes running on common vLLM/Beam ports or named vllm\n",
        "    try:\n",
        "        # This finds any process names containing 'vllm' or 'python' and kills them\n",
        "        # excluding the current notebook process\n",
        "        os.system(\"pkill -f vllm\")\n",
        "        print(\"Killed existing vLLM processes.\")\n",
        "    except Exception as e:\n",
        "        print(f\"No processes to kill: {e}\")\n",
        "\n",
        "    # 2. Clear PyTorch Cache\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    print(\"GPU Memory cleared.\")\n",
        "\n",
        "cleanup_vllm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzGvADQPpGip",
        "outputId": "c0a75e96-c0e6-4830-db8d-64273a52e730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Killed existing vLLM processes.\n",
            "GPU Memory cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This should not be necessary outside of colab. Specifically for apache_beam\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "-xwU5dWNXRyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from apache_beam.ml.inference.base import RunInference\n",
        "from apache_beam.ml.inference.vllm_inference import VLLMCompletionsModelHandler\n",
        "from apache_beam.ml.inference.base import PredictionResult\n",
        "from apache_beam.ml.inference import vllm_inference\n",
        "import apache_beam as beam\n",
        "import time"
      ],
      "metadata": {
        "id": "W7vhISQ-Xe3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prefix Caching\n",
        "\n",
        "The Prefill Bottleneck In Large Language Model (LLM) inference is that every request begins with a prefill phase, where the model \"reads\" and processes the entire input prompt to generate its internal mathematical representation, known as the KV (Key-Value) Cache. When multiple users interact with the same long document, or when an AI agent sends a repeated \"System Prompt\" (e.g., a 2,000-token instruction set) with every query, the GPU is forced to redundantly re-calculate the same KV cache for that shared text every single time.\n",
        "\n",
        "This bottleneck is caused by the stateless nature of standard transformers, which lack a native \"memory\" of previous computations. This results in high latency, wasted compute cycles, and significant energy inefficiency, especially as context windows grow into the tens of thousands of tokens.\n",
        "\n",
        "\n",
        "The Solution: Automatic Prefix Caching (APC) Prefix Caching solves this by treating the KV cache as a reusable asset. Using a technique like **PagedAttention**, the engine partitions the KV cache into fixed-size blocks and stores them in GPU memory. When a new request arrives, the engine performs a cryptographic hash of the input tokens; if the \"prefix\" matches a hash already stored in memory, the model skips the prefill computation entirely for that segment. It simply \"points\" to the existing memory blocks and moves directly to the decode phase (generating new text).\n",
        "\n",
        "Essentially, the server that hosts the LLM keeps track of as many previous KV-cache blocks as possible, using the Least Recently Used method. These blocks and hashes also do not need to be seperated by user, as the hashes match if that section of the prompt matches. This saves a lot of computation. Below, it performed roughly 10% better on less than 4,000 tokens.\n"
      ],
      "metadata": {
        "id": "-cpQUarV8Int"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RA9KZWS3jQJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate a document with thousands of tokens\n",
        "CONTEXT = \"Context: \" + (\"This is important data about L4 GPUs. \" * 400)\n",
        "\n",
        "# Ask many different specific questions about that one document\n",
        "prompts = [f\"{CONTEXT} Question: What is mentioned in paragraph {i}?\" for i in range(80)]\n",
        "\n",
        "class ProcessStats(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        # 1. Inspect the object\n",
        "        # print the full structure if needed\n",
        "        # print(f\"DEBUG: Full element: {element}\")\n",
        "        # print(f\"DEBUG: Inference object type: {type(element.inference)}\")\n",
        "\n",
        "        response_text = element.inference.choices[0].text\n",
        "        gen_tokens = element.inference.usage.completion_tokens\n",
        "        yield gen_tokens\n",
        "\n",
        "def run_benchmark(enable_caching):\n",
        "    # Ensure all values are strings to satisfy the Beam CLI builder\n",
        "    config = {\n",
        "        \"model\": \"Qwen/Qwen2.5-7B-Instruct-AWQ\",\n",
        "        \"enable_prefix_caching\": str(enable_caching),\n",
        "        \"gpu_memory_utilization\": \"0.8\",\n",
        "        \"max_model_len\": \"4096\",\n",
        "    }\n",
        "\n",
        "    model_handler = vllm_inference.VLLMCompletionsModelHandler(\n",
        "        model_name='Qwen/Qwen2.5-7B-Instruct-AWQ',\n",
        "        vllm_server_kwargs=config\n",
        "    )\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    with beam.Pipeline() as p:\n",
        "        # Collect the counts to force execution and calculate total tokens\n",
        "        counts = (\n",
        "            p\n",
        "            | \"Create\" >> beam.Create(prompts)\n",
        "            | \"Inference\" >> RunInference(model_handler)\n",
        "            | \"Stats\" >> beam.ParDo(ProcessStats())\n",
        "        )\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    total_duration = end_time - start_time\n",
        "    return total_duration\n",
        "\n",
        "# --- Execution ---\n",
        "cleanup_vllm()\n",
        "print(\"--- Starting Baseline ---\")\n",
        "time_baseline = run_benchmark(False)\n",
        "\n",
        "# cleanup function so the L4 is ready for the next server start.\n",
        "time.sleep(5)\n",
        "cleanup_vllm()\n",
        "\n",
        "print(\"--- Starting Cached ---\")\n",
        "time_cached = run_benchmark(True)\n",
        "\n",
        "# 2. Final Stats Output\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"RESULTS FOR L4 GPU\")\n",
        "print(f\"Baseline Total Time: {time_baseline:.2f}s\")\n",
        "print(f\"Cached Total Time:   {time_cached:.2f}s\")\n",
        "print(f\"Speedup Factor:      {time_baseline / time_cached:.2f}x\")\n",
        "# Assuming approx 150 tokens generated across all prompts for the T/S estimate\n",
        "print(f\"Est. Tokens/Sec (Baseline): {150 / time_baseline:.2f}\")\n",
        "print(f\"Est. Tokens/Sec (Cached):   {150 / time_cached:.2f}\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9_n9vLCbVrE",
        "outputId": "fd7ad415-a28a-4bee-e1a5-c25e975251a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Killed existing vLLM processes.\n",
            "GPU Memory cleared.\n",
            "--- Starting Baseline ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/apache_beam/transforms/util.py:477: RankWarning: Polyfit may be poorly conditioned\n",
            "  b, a = np.polyfit(xs, ys, 1, w=weight)\n",
            "/usr/local/lib/python3.12/dist-packages/apache_beam/transforms/util.py:477: RankWarning: Polyfit may be poorly conditioned\n",
            "  b, a = np.polyfit(xs, ys, 1, w=weight)\n",
            "/usr/local/lib/python3.12/dist-packages/apache_beam/transforms/util.py:477: RankWarning: Polyfit may be poorly conditioned\n",
            "  b, a = np.polyfit(xs, ys, 1, w=weight)\n",
            "/usr/local/lib/python3.12/dist-packages/apache_beam/transforms/util.py:477: RankWarning: Polyfit may be poorly conditioned\n",
            "  b, a = np.polyfit(xs, ys, 1, w=weight)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Killed existing vLLM processes.\n",
            "GPU Memory cleared.\n",
            "--- Starting Cached ---\n",
            "\n",
            "========================================\n",
            "RESULTS FOR L4 GPU\n",
            "Baseline Total Time: 126.85s\n",
            "Cached Total Time:   115.36s\n",
            "Speedup Factor:      1.10x\n",
            "Est. Tokens/Sec (Baseline): 1.18\n",
            "Est. Tokens/Sec (Cached):   1.30\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunked Prefill\n",
        "\n",
        "The \"Prefill Stall\": When an LLM receives a prompt, it must first read the entire input before it can begin writing the first word of the response. This initial reading phase is called the prefill. For large prompts the prefill requires a burst of GPU computation. In standard systems, this creates a stall where the GPU is 100% occupied with reading a new request and cannot generate tokens for any other active requests. If multiple users are on the same GPU, one personâ€™s giant document can effectively freeze the experience for everyone else, leading to jerky, inconsistent output speeds and high latency.\n",
        "\n",
        "\n",
        "The Solution: Granular Scheduling Chunked Prefill solves this by breaking the massive \"reading\" task into smaller, manageable pieces (chunks). Instead of processing a 4,000-token prompt in one giant block, the system can process it in eight 512-token chunks. Between these chunks, the GPU can \"context switch\" to perform other work, such as generating the next token for a different user. If another person's prompt is less than 512 tokens, theirs can get an answer between the larger prompt.\n",
        "\n",
        "A pipeline looks like this: your prompt reaches the LLM server. It gets tokenized and matched with embeddings. Now, it is taken is chunks and run through the transformer architecture. The final representations are stored as KV-cache. Then other prompts are also looked at. Once your prompt is finished going through the tranformer, the next token is predicted. Attention is still spanning the entire input, not just the chunks. This is because as each chunk is processed, it is looking back to all previous chunks due to the causal mask.\n",
        "\n",
        "In the code below, chunked prefix is seen through the VLLM library. As can be seen, both the victim and the blockers time improved. This is an interesting effect of using the GPU and KV-cache properly."
      ],
      "metadata": {
        "id": "BGK_BSfFCcmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import threading\n",
        "import signal\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct-AWQ\"\n",
        "PORT = \"8000\"\n",
        "# Massive document to ensure the Blocker takes significant time\n",
        "HUGE_DOC = \"Context: \" + (\"This is a long document for sequential vs interleaved test. \" * 3500)\n",
        "SMALL_PROMPT = \"What is 2+2?\"\n",
        "\n",
        "def wait_for_server():\n",
        "    for i in range(150):\n",
        "        try:\n",
        "            res = requests.get(f\"http://localhost:{PORT}/v1/models\")\n",
        "            if res.status_code == 200: return True\n",
        "        except: time.sleep(2)\n",
        "    return False\n",
        "\n",
        "def send_request(prompt, label, results):\n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        payload = {\"model\": MODEL_NAME, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"max_tokens\": 10, \"stream\": True}\n",
        "        response = requests.post(f\"http://localhost:{PORT}/v1/chat/completions\", json=payload, timeout=180, stream=True)\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                results[label] = time.perf_counter() - start\n",
        "                break\n",
        "    except Exception as e: print(f\"[{label}] Failed: {e}\")\n",
        "\n",
        "def run_test(mode):\n",
        "    print(f\"\\n{'='*20}\\nMODE: {mode}\\n{'='*20}\")\n",
        "\n",
        "    # Configuration based on your request\n",
        "    if mode == \"Sequential (Normal)\":\n",
        "        cmd_args = [\"--max-num-seqs\", \"1\"]\n",
        "    else: # Interleaved (Chunked)\n",
        "        cmd_args = [\n",
        "            \"--enable-chunked-prefill\", \"true\",\n",
        "            \"--max-num-batched-tokens\", \"256\",\n",
        "            \"--max-num-seqs\", \"2\"\n",
        "        ]\n",
        "\n",
        "    cmd = [\n",
        "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
        "        \"--model\", MODEL_NAME, \"--port\", PORT,\n",
        "        \"--gpu-memory-utilization\", \"0.85\",\n",
        "        \"--max-model-len\", \"32768\",\n",
        "    ] + cmd_args\n",
        "\n",
        "    proc = subprocess.Popen(cmd, preexec_fn=os.setsid, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    if not wait_for_server():\n",
        "        print(\"Server failed to start.\")\n",
        "        return None\n",
        "    print(\"Server is READY!\")\n",
        "\n",
        "    results = {}\n",
        "    t1 = threading.Thread(target=send_request, args=(HUGE_DOC, \"Blocker\", results))\n",
        "    t2 = threading.Thread(target=send_request, args=(SMALL_PROMPT, \"Victim\", results))\n",
        "\n",
        "    t1.start()\n",
        "    time.sleep(2.0) # Ensure Blocker is deep in its work\n",
        "    t2.start()\n",
        "\n",
        "    t1.join()\n",
        "    t2.join()\n",
        "\n",
        "    # Calculate finish times relative to the start of the test\n",
        "    b_ttft = results.get(\"Blocker\", 0)\n",
        "    v_ttft = results.get(\"Victim\", 0)\n",
        "\n",
        "    print(f\"[Blocker] TTFT: {b_ttft:.2f}s\")\n",
        "    print(f\"[Victim]  TTFT: {v_ttft:.2f}s\")\n",
        "\n",
        "    os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n",
        "    !pkill -9 -f vllm\n",
        "    time.sleep(5)\n",
        "    return v_ttft\n",
        "\n",
        "# --- EXECUTION ---\n",
        "!pkill -9 -f vllm\n",
        "\n",
        "# Run 1: Sequential (Normal)\n",
        "v_sequential = run_test(\"Sequential (Normal)\")\n",
        "\n",
        "# Run 2: Interleaved (Chunked)\n",
        "v_interleaved = run_test(\"Interleaved (Chunked)\")\n",
        "\n",
        "# --- FINAL ANALYSIS ---\n",
        "print(\"\\n\" + \"#\"*50)\n",
        "print(\"COMPARISON: SEQUENTIAL VS. INTERLEAVED\")\n",
        "print(f\"Victim Wait (Sequential):  {v_sequential:.2f}s (Stuck behind Blocker's entire job)\")\n",
        "print(f\"Victim Wait (Interleaved): {v_interleaved:.2f}s (Cut in between chunks)\")\n",
        "\n",
        "if v_interleaved > 0:\n",
        "    speedup = v_sequential / v_interleaved\n",
        "    print(f\"Efficiency Gain:           {speedup:.1f}x faster response\")\n",
        "print(\"#\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUreCnkC47sU",
        "outputId": "c47f61d4-58c6-46a9-f34e-a19cccf72956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================\n",
            "MODE: Sequential (Normal)\n",
            "====================\n",
            "Server is READY!\n",
            "[Blocker] TTFT: 3.69s\n",
            "[Victim]  TTFT: 1.72s\n",
            "\n",
            "====================\n",
            "MODE: Interleaved (Chunked)\n",
            "====================\n",
            "Server is READY!\n",
            "[Blocker] TTFT: 2.89s\n",
            "[Victim]  TTFT: 0.93s\n",
            "\n",
            "##################################################\n",
            "COMPARISON: SEQUENTIAL VS. INTERLEAVED\n",
            "Victim Wait (Sequential):  1.72s (Stuck behind Blocker's entire job)\n",
            "Victim Wait (Interleaved): 0.93s (Cut in between chunks)\n",
            "Efficiency Gain:           1.8x faster response\n",
            "##################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concurrency Stress\n",
        "\n"
      ],
      "metadata": {
        "id": "KRvdHtkiOztb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 3: CONCURRENCY STRESS ---\n",
        "prompts_stress = [f\"Explain concept {i} in 50 words.\" for i in range(150)]\n",
        "\n",
        "def run_stress_benchmark(high_concurrency):\n",
        "    config = {\n",
        "        \"model\": \"Qwen/Qwen2.5-7B-Instruct-AWQ\",\n",
        "        \"gpu_memory_utilization\": \"0.9\" if high_concurrency else \"0.5\",\n",
        "        \"max_num_seqs\": \"128\" if high_concurrency else \"16\", # vLLM dynamic batching\n",
        "        \"max_model_len\": \"2048\",\n",
        "    }\n",
        "    model_handler = vllm_inference.VLLMCompletionsModelHandler(\n",
        "        model_name='Qwen/Qwen2.5-7B-Instruct-AWQ',\n",
        "        vllm_server_kwargs=config\n",
        "    )\n",
        "    start_time = time.perf_counter()\n",
        "    with beam.Pipeline() as p:\n",
        "        _ = (p | \"Create\" >> beam.Create(prompts_stress)\n",
        "               | \"Inference\" >> RunInference(model_handler)\n",
        "               | \"Stats\" >> beam.ParDo(ProcessStats()))\n",
        "    return time.perf_counter() - start_time\n",
        "\n",
        "cleanup_vllm()\n",
        "print(\"--- Stress: Starting Baseline (Low Concurrency) ---\")\n",
        "time_stress_base = run_stress_benchmark(False)\n",
        "\n",
        "time.sleep(5)\n",
        "cleanup_vllm()\n",
        "\n",
        "print(\"--- Stress: Starting vLLM Optimized (High Concurrency) ---\")\n",
        "time_stress_vllm = run_stress_benchmark(True)\n",
        "\n",
        "print(f\"\\nSTRESS RESULTS: Speedup {time_stress_base / time_stress_vllm:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTE1Fm5l7kOP",
        "outputId": "57c4afc6-037f-48a6-b098-c838775e2115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Killed existing vLLM processes.\n",
            "GPU Memory cleared.\n",
            "--- Stress: Starting Baseline (Low Concurrency) ---\n",
            "Killed existing vLLM processes.\n",
            "GPU Memory cleared.\n",
            "--- Stress: Starting vLLM Optimized (High Concurrency) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/apache_beam/transforms/util.py:477: RankWarning: Polyfit may be poorly conditioned\n",
            "  b, a = np.polyfit(xs, ys, 1, w=weight)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STRESS RESULTS: Speedup 1.09x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram Speculative Decoding\n",
        "\n",
        "This is a technique used to accelerate the inference of Large Model Models (LLMs) by matching words found in the original prompt and then taking the next few words and getting them checked by the actual LLM.\n",
        "\n",
        "In standard inference, a model must perform a full forward pass to generate every single token, which is often bottlenecked by memory bandwidth. N-gram speculation solves this by predicting that the model will repeat sequences it has already generated within the current prompt. It works by maintaining a local window of recently generated tokens and searching the previous context for matches. When a matching \"n-gram\" (a sequence of n tokens) is found, the system \"speculates\" that the tokens following that previous occurrence will appear again. These speculated tokens are then verified by the LLM in a single parallel forward pass. If the speculation is correct, the model can \"produce\" multiple tokens in the time it would usually take to generate one; if incorrect, the model simply reverts to its standard output with negligible computational overhead.\n",
        "\n",
        "It is important to note that the large LLM one can go through these newly generated tokens in a single forward pass, by treating them as part of the original prompt and applying a causal attention mask to it to generate probabilities of what it (the larger model) would have generated at each point. If the tokens match, it is accepted. If they don't, the probability distribution of the larger LLM decides the token (taking temperature, etc. into account), and the rest of the newly generated ones are discarded. Then the smaller model predicts once again.\n",
        "\n"
      ],
      "metadata": {
        "id": "VrXlXj4kMpuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import signal\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# --- CONFIG ---\n",
        "MODEL = \"Qwen/Qwen2.5-7B-Instruct-AWQ\"\n",
        "PORT = \"8000\"\n",
        "\n",
        "# A massive, highly repetitive technical/legal block.\n",
        "REPETITIVE_BLOCK = (\n",
        "    \"Compliance Requirement Alpha-7: All data must be encrypted using AES-256 at rest and TLS 1.3 in transit. \"\n",
        "    \"Security Protocol Beta-9: Access logs must be retained for 365 days in a read-only bucket. \"\n",
        "    \"System Architecture Gamma-3: Load balancers must be configured for geographical redundancy across three regions. \"\n",
        ") * 60  # ~4,000+ tokens of predictable patterns\n",
        "\n",
        "PROMPT = f\"{REPETITIVE_BLOCK}\\n\\nBased on the technical manual above, list the exact encryption requirements for Requirement Alpha-7, the retention period for Beta-9, and the redundancy plan for Gamma-3. Copy the phrases exactly.\"\n",
        "\n",
        "def cleanup():\n",
        "    print(\"\\n[System] Clearing GPU & CPU RAM...\")\n",
        "    os.system(\"pkill -9 -f vllm\")\n",
        "    os.system(\"fuser -k /dev/nvidia0 2>/dev/null\")\n",
        "    torch.cuda.empty_cache()\n",
        "    time.sleep(5)\n",
        "\n",
        "def run_server(mode_name, spec_config=None):\n",
        "    cleanup()\n",
        "    cmd = [\n",
        "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
        "        \"--model\", MODEL,\n",
        "        \"--port\", PORT,\n",
        "        \"--gpu-memory-utilization\", \"0.85\",\n",
        "        \"--enforce-eager\",\n",
        "        \"--max-model-len\", \"8192\",\n",
        "        \"--trust-remote-code\"\n",
        "    ]\n",
        "    if spec_config:\n",
        "        cmd += [\"--speculative-config\", json.dumps(spec_config)]\n",
        "\n",
        "    print(f\"--- Launching {mode_name} ---\")\n",
        "    log_f = open(f\"vllm_{mode_name}.log\", \"w\")\n",
        "    proc = subprocess.Popen(cmd, stdout=log_f, stderr=log_f, preexec_fn=os.setsid)\n",
        "\n",
        "    for i in range(60):\n",
        "        try:\n",
        "            if requests.get(f\"http://localhost:{PORT}/v1/models\", timeout=1).status_code == 200:\n",
        "                print(f\" {mode_name} ONLINE!\")\n",
        "                return proc\n",
        "        except:\n",
        "            time.sleep(2)\n",
        "    return None\n",
        "\n",
        "def benchmark(name):\n",
        "    print(f\"Benchmarking {name}...\")\n",
        "    start = time.perf_counter()\n",
        "    resp = requests.post(f\"http://localhost:{PORT}/v1/chat/completions\",\n",
        "                        json={\n",
        "                            \"model\": MODEL,\n",
        "                            \"messages\": [{\"role\":\"user\", \"content\": PROMPT}],\n",
        "                            \"max_tokens\": 400,\n",
        "                            \"temperature\": 0\n",
        "                        })\n",
        "    duration = time.perf_counter() - start\n",
        "    tokens = resp.json()['usage']['completion_tokens']\n",
        "    tps = tokens / duration\n",
        "    print(f\"Result: {tps:.2f} tokens/sec\")\n",
        "    return tps\n",
        "\n",
        "# --- EXECUTION ---\n",
        "v_tps = benchmark(\"Vanilla\") if run_server(\"Vanilla\") else 0\n",
        "\n",
        "# N-gram Configuration: We increase speculative tokens to 10\n",
        "# to take advantage of the high certainty in this text.\n",
        "ngram_cfg = {\n",
        "    \"method\": \"ngram\",\n",
        "    \"num_speculative_tokens\": 10,  # Predicting more tokens at once\n",
        "    \"ngram_prompt_lookup_max\": 4\n",
        "}\n",
        "n_tps = benchmark(\"N-Gram\") if run_server(\"N-Gram\", ngram_cfg) else 0\n",
        "\n",
        "if v_tps and n_tps:\n",
        "    print(f\"\\n{'='*40}\\nFINAL SPEEDUP: {((n_tps/v_tps)-1)*100:.1f}%\\n{'='*40}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_8QLvsyMwEl",
        "outputId": "82a583f7-e3bd-4597-d427-38a5eef8239c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[System] Clearing GPU & CPU RAM...\n",
            "--- Launching Vanilla ---\n",
            " Vanilla ONLINE!\n",
            "Benchmarking Vanilla...\n",
            "Result: 21.21 tokens/sec\n",
            "\n",
            "[System] Clearing GPU & CPU RAM...\n",
            "--- Launching N-Gram ---\n",
            " N-Gram ONLINE!\n",
            "Benchmarking N-Gram...\n",
            "Result: 22.45 tokens/sec\n",
            "\n",
            "========================================\n",
            "FINAL SPEEDUP: 5.8%\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard Speculative Decoding\n",
        "\n",
        "This uses 2 models: a smaller draft, and a larger target.\n",
        "\n",
        "The smaller lightweight 'draft' LLm is to predict the next few words. Because this model is small, its inference is significantly faster and less memory-intensive compared to the larger one. With good models, this can achieve an accuracy of more than 50%, saving a lot of time and compute by not needing the target LLM to produce these tokens.\n",
        "\n",
        "These newly predicted candidates are then passed to the large target model, which verifies the entire batch in a single, parallel forward pass. This process, often combined with rejection sampling, ensures that the final output remains identical in quality to what the large model would have produced alone.\n",
        "\n",
        "It is important to note that the large LLM one can go through these newly generated tokens in a single forward pass, by treating them as part of the original prompt and applying a causal attention mask to it to generate probabilities of what it (the larger model) would have generated at each point. If the tokens match, it is accepted. If they don't, the probability distribution of the larger LLM decides the token (taking temperature etc into account), and the rest of the newly generated ones are discarded. Then the smaller model predicts once again."
      ],
      "metadata": {
        "id": "VlALUdCAWOsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medusa/Eagle Speculative Decoding\n",
        "\n",
        "In this type of technique, a single LLM tries to make predictions multiple tokens ahead, then uses these to double check. This is done by attaching speculative 'heads' to the end of the model. These 'heads' are lightweight layers/transformer blocks they do not take much compute or memory. These layers are specifically trained to predict ahead.\n",
        "\n",
        "Medusa's approach was predicting by looking at the hidden previous layers states and go from there.\n",
        "\n",
        "Eagle improved upon this by adding feature extrapolation. This takes the current hidden state of a model and predicts what the next one might be.\n",
        "\n",
        "These methods usually provide top-k choices for their predictions. To validate these candidates, the target LLM now uses a tree attention mask in a single forward pass."
      ],
      "metadata": {
        "id": "Fy5UH413oYT0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MncblbHLoTds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import signal\n",
        "import torch\n",
        "\n",
        "# Example Comparison between draft model and bigger model output\n",
        "\n",
        "# --- CONFIG ---\n",
        "DRAFT_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "LARGE_MODEL = \"Qwen/Qwen2.5-7B-Instruct-AWQ\"\n",
        "PORT = \"8000\"\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"Forcefully clears GPU memory and kills lingering vLLM processes.\"\"\"\n",
        "    print(\"\\n[Cleanup] Cleaning up GPU and processes...\")\n",
        "    # Kill vLLM and any process using the GPU\n",
        "    os.system(\"pkill -9 -f vllm\")\n",
        "    os.system(\"fuser -k /dev/nvidia0\") # Kills any process touching GPU 0\n",
        "\n",
        "    # Clear Python's internal cache\n",
        "    torch.cuda.empty_cache()\n",
        "    time.sleep(5) # Essential for the driver to reset\n",
        "\n",
        "def wait_for_server(log_file=\"vllm_logs.txt\"):\n",
        "    print(\"Polling server (L4 optimized)\", end=\"\")\n",
        "    for i in range(120):\n",
        "        try:\n",
        "            res = requests.get(f\"http://localhost:{PORT}/v1/models\", timeout=1)\n",
        "            if res.status_code == 200:\n",
        "                print(\" ONLINE!\")\n",
        "                return True\n",
        "        except:\n",
        "            if os.path.exists(log_file):\n",
        "                with open(log_file, \"r\") as f:\n",
        "                    content = f.read()\n",
        "                    if \"Error\" in content or \"RuntimeError\" in content:\n",
        "                        print(f\"\\nCRASH DETECTED:\\n{content[-500:]}\")\n",
        "                        return False\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(2)\n",
        "    return False\n",
        "\n",
        "def start_vllm_l4(model, gpu_util):\n",
        "    # Added --kv-cache-dtype fp8 to save massive VRAM on L4\n",
        "    # Added --gpu-memory-utilization adjustment\n",
        "    cmd = [\n",
        "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
        "        \"--model\", model,\n",
        "        \"--port\", PORT,\n",
        "        \"--gpu-memory-utilization\", str(gpu_util),\n",
        "        \"--enforce-eager\",\n",
        "        \"--kv-cache-dtype\", \"fp8\",\n",
        "        \"--trust-remote-code\",\n",
        "        \"--max-model-len\", \"4096\"\n",
        "    ]\n",
        "\n",
        "    log_f = open(\"vllm_logs.txt\", \"w\")\n",
        "    proc = subprocess.Popen(cmd, preexec_fn=os.setsid, stdout=log_f, stderr=log_f)\n",
        "    return proc, log_f\n",
        "\n",
        "# --- EXECUTION ---\n",
        "cleanup()\n",
        "\n",
        "# --- STEP 1: DRAFT ---\n",
        "print(f\"\\n[Step 1] Launching Draft: {DRAFT_MODEL}\")\n",
        "proc, logs = start_vllm_l4(DRAFT_MODEL, 0.25)\n",
        "\n",
        "draft_txt = \"\"\n",
        "if wait_for_server():\n",
        "    res = requests.post(f\"http://localhost:{PORT}/v1/chat/completions\",\n",
        "                        json={\"model\": DRAFT_MODEL, \"messages\": [{\"role\":\"user\", \"content\":\"Write a 1-sentence sci-fi hook.\"}], \"max_tokens\":30})\n",
        "    draft_txt = res.json()['choices'][0]['message']['content']\n",
        "    print(f\"\\nDraft Result: {draft_txt}\")\n",
        "\n",
        "# --- TRANSITION ---\n",
        "# Explicitly close logs and kill process group\n",
        "os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n",
        "logs.close()\n",
        "cleanup() # Reset GPU for the big model\n",
        "\n",
        "# --- STEP 2: LARGE ---\n",
        "print(f\"\\n[Step 2] Launching Large Model: {LARGE_MODEL}\")\n",
        "# Lowered to 0.65 to ensure it fits comfortably within the L4 overhead\n",
        "proc, logs = start_vllm_l4(LARGE_MODEL, 0.65)\n",
        "\n",
        "if wait_for_server():\n",
        "    res = requests.post(f\"http://localhost:{PORT}/v1/chat/completions\",\n",
        "                        json={\"model\": LARGE_MODEL, \"messages\": [{\"role\":\"user\", \"content\": f\"Expand this into a short paragraph: {draft_txt}\"}], \"max_tokens\":150})\n",
        "    print(f\"\\nFinal Result: {res.json()['choices'][0]['message']['content']}\")\n",
        "\n",
        "# Final Cleanup\n",
        "os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n",
        "logs.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "semvD-cr6YKn",
        "outputId": "9ba9bbed-9909-4ae2-aec0-bb5e7a431557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Cleanup] Cleaning up GPU and processes...\n",
            "\n",
            "[Step 1] Launching Draft: Qwen/Qwen2.5-0.5B-Instruct\n",
            "Polling server (L4 optimized)...................................... ONLINE!\n",
            "\n",
            "Draft Result: In the vast expanse of space, a team of explorers discovers an ancient alien city hidden deep within the Earth's crust.\n",
            "\n",
            "[Cleanup] Cleaning up GPU and processes...\n",
            "\n",
            "[Step 2] Launching Large Model: Qwen/Qwen2.5-7B-Instruct-AWQ\n",
            "Polling server (L4 optimized)............................................. ONLINE!\n",
            "\n",
            "Final Result: In the vast expanse of the rugged, mountainous terrain, a team of intrepid explorers stumbled upon something utterly unexpectedâ€” a sprawling ancient alien city, hidden deep within the Earth's crust. The city, with its towering structures-like structures and intricate carvv patterns, seemed to have been been beenveriously carved eons ago. The team of explorers, excited yet yet fearful, carefully ventured into the heart of of the ancient alien city, hoping to uncover its secrets and learn perhaps lost they long revealing it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rhLv9mnptKxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}